{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HN_SentimentAnalysis_v2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marinathomas/SentimentAnalysisHN/blob/master/HN_SentimentAnalysis_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3KSAt8AZ99W",
        "colab_type": "text"
      },
      "source": [
        "1. Load credentials to access BigQuery\n",
        "2. Read the story ids for 2017 from the 'full' table.\n",
        "3. For each story, get the associated 'main' comments. We are not considering response to the comments for now.\n",
        "4. Analyze the comments and give the story a score based off the sentiment of the comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoeDjIX-akYF",
        "colab_type": "text"
      },
      "source": [
        "Step 1 - Load credentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQp6b_UOhDXd",
        "colab_type": "code",
        "outputId": "e6c9d9d0-5032-4514-d156-2e02e6426f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGloHhbS8Neq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = \"/content/gdrive/Shared drives/HackerNews:SentimentAnalysis/\"\n",
        "credential_path = root_path+\"hackernews-bigquery-261019-0f8cc2295b63.json\"\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMk5ahU8emp9",
        "colab_type": "text"
      },
      "source": [
        "Step 2 - Install the sentiment analysis library\n",
        "https://github.com/cjhutto/vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FMASZlGpLX",
        "colab_type": "code",
        "outputId": "b1f41b91-a535-4395-a568-8b21ba6c5bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PIfMSlUq0Aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3GPA8d_gG70",
        "colab_type": "text"
      },
      "source": [
        "Step 3 - Load BigQuery client and HackerNews dataset\n",
        "1. Load the BigQuery client\n",
        "2. Get a reference to HackerNews dataset\n",
        "3. Load the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i3BPczYgKOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "client = bigquery.Client()\n",
        "hn_dataset_ref = client.dataset('hacker_news', project='bigquery-public-data')\n",
        "hn_dset = client.get_dataset(hn_dataset_ref)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgFk8Np_hv0b",
        "colab_type": "text"
      },
      "source": [
        "Step 4 - Look for stories of 2017"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pz4ABH68SCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stories():\n",
        "    query = \"\"\"\n",
        "    SELECT table_full.id, table_full.title, table_full.url, table_full.score, table_full.descendants\n",
        "    FROM `bigquery-public-data.hacker_news.full` as table_full\n",
        "    WHERE  table_full.type = 'story' and REGEXP_CONTAINS(title, r\"(S|s)how HN\") and (deleted IS NULL or deleted IS FALSE) and  EXTRACT(YEAR FROM timestamp)=2017\n",
        "    and table_full.descendants > 5\n",
        "    --and (table_full.descendants = 0 or table_full.descendants <5)\n",
        "    ORDER BY id asc\n",
        "    --limit 50\n",
        "    \"\"\"\n",
        "\n",
        "    query_job = client.query(query)\n",
        "    iterator = query_job.result(timeout=30)\n",
        "    rows = list(iterator)\n",
        "\n",
        "    # Transform the rows into a nice pandas dataframe\n",
        "    stories = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n",
        "    #stories.head(10)\n",
        "\n",
        "    return stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7BD3HNff6dc",
        "colab_type": "text"
      },
      "source": [
        "Lets check the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvQed0Shpypt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "stories = get_stories()\n",
        "csv_file = root_path + \"stories_with_more_than_5_comments_2017.csv\"\n",
        "with open(csv_file, mode='w') as stories_file:\n",
        "  stories_writer = csv.writer(stories_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  for index,row in stories.iterrows():\n",
        "      title, parent_id, score, noOf_comments, url = row['title'], row['id'], row['score'],row['descendants'], row['url']\n",
        "      stories_writer.writerow([index, title, parent_id, url, noOf_comments])\n",
        "      #print(\"----------------------------------------\")\n",
        "      #print('{} Title: {} \\t  ID: {} \\t url: {} \\t descendants:{}'.format(index, title, parent_id, url, noOf_comments))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZTiPg2diEXk",
        "colab_type": "text"
      },
      "source": [
        "Let's bring up the comments for the above stories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JKtiaSaa-zg",
        "colab_type": "text"
      },
      "source": [
        "Step 5 - For each story, bring up the associated comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tQ7FmpX8S6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_comments(parent_id):\n",
        "    query = \"\"\"\n",
        "    select  table_full.id, table_full.text\n",
        "    from `bigquery-public-data.hacker_news.full`  as table_full\n",
        "    where type = 'comment'  and (deleted IS NULL or deleted IS FALSE) and parent = @parent\n",
        "    order by parent ;\n",
        "    \"\"\"\n",
        "\n",
        "    query_params = [\n",
        "        bigquery.ScalarQueryParameter(\"parent\", \"INT64\", parent_id)\n",
        "    ] \n",
        "\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    job_config.query_parameters = query_params\n",
        "    query_job = client.query(query,location=\"US\",job_config=job_config,)  \n",
        "\n",
        "    iterator = query_job.result(timeout=30)\n",
        "    rows = list(iterator)\n",
        "\n",
        "    comments = pd.DataFrame()\n",
        "    # Transform the rows into a nice pandas dataframe\n",
        "    if rows:\n",
        "      comments_temp = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n",
        "      comments.append(comments_temp)\n",
        "      #comments.head(20)\n",
        "\n",
        "    return comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWnSGti1fb5l",
        "colab_type": "text"
      },
      "source": [
        "Step 6 - Analyse comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B99QrxLgUsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, row in stories.iterrows():\n",
        "  parent_id = 14497295 #row['id']\n",
        "  comments = get_comments(parent_id)\n",
        "  analyse_comments(comments, parent_id)\n",
        "  break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCb90L1uL0H5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyse_comments(comments, parent_id):\n",
        "  scores = []\n",
        "  csv_file = root_path + \"sentiment_analysis_results/comments_for_\"+str(parent_id)+\".csv\"\n",
        "\n",
        "  with open(csv_file, mode='w') as comments_file:\n",
        "    comments_writer = csv.writer(comments_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    for index,row in comments.iterrows():\n",
        "        sentence = row['text']\n",
        "        score = analyser.polarity_scores(str(sentence))\n",
        "        print(\"{}\\t Comment: {} \\t SCORE: {}\".format(index, sentence, str(score)))\n",
        "        comments_writer.writerow([index, sentence, str(score)])\n",
        "        scores.append(score)\n",
        "  return scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxvy0358Z3y",
        "colab_type": "text"
      },
      "source": [
        "TODO: Analyze comments of comments ==> Will take up later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er6ATerIzh9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_story(scores):\n",
        "  story_score = 0\n",
        "  for row in scores:\n",
        "    compound_score = row['compound']\n",
        "    if compound_score >= 0.05:\n",
        "      story_score += 1\n",
        "    elif compound_score <= -0.05:\n",
        "      story_score -= 1\n",
        "  return story_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5DLqcGd8giv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = root_path + \"sentiment_analysis_results/scores_stories_with_comments_2017.csv\"\n",
        "\n",
        "def analyse_hacker_news():\n",
        "  stories = get_stories()\n",
        "  scored_stories = []\n",
        "  for index, row in stories.iterrows():\n",
        "    parent_id = row['id']\n",
        "    comments = get_comments(parent_id)\n",
        "    story_point=0\n",
        "    if not comments.empty:\n",
        "      scores = analyse_comments(comments, parent_id)\n",
        "      story_point = score_story(scores)\n",
        "    with open(csv_file, mode='a') as scores_file:\n",
        "      scores_writer = csv.writer(scores_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      scores_writer.writerow([index, row['title'], parent_id, row['url'], row['score'],row['descendants'],story_point])\n",
        "    print(\"{} Story {} with id {} and noOf_Comments {} url {} with original score O_Score {} scored {}\".format(index, row['title'], parent_id, row['descendants'], row['url'], row['score'], story_point))\n",
        "    break;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3L55tVC3cCL",
        "colab_type": "code",
        "outputId": "f2dd782e-3afb-43de-dddf-0e46670dcff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "analyse_hacker_news()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Story Show HN: Blink my keyboard lights when you visit this page with id 13293894 and noOf_Comments 22 url http://lelandbatey.com/posts/2016/12/Making-lights-blink-for-each-HTTP-request/ with original score O_Score 87 scored 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}